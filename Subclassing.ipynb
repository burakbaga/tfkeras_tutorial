{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdbcdd54-3a39-44f0-999d-6cb23ae9fd7c",
   "metadata": {},
   "source": [
    "# Making new layers and models via subclassing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f919aca-65c2-4edc-908a-babdd9e99528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e65116-5bb5-46b5-835a-7bbdf996c649",
   "metadata": {},
   "source": [
    "# Layer Sınıfı : Weightlerler ve Hesaplamalar\n",
    "Kerastaki en temel abstractionlardan biri Layer sınıfıdır. Layer sınıfı state(weights) ve inputtan outputa transformasyonu(call metodu ile) encapsulate eder.  \n",
    "Aşağıda bir dense layer oluşturalım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd00dcf-2598-4bf1-b25e-1e240349e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self,units=32,input_dim=32):\n",
    "        super(Linear,self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim,units),dtype=\"float32\"),\n",
    "                            trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),dtype=\"float32\"),trainable=True)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a802376-9777-409b-8a11-b84338f575bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.06484985  0.09588623  0.11465454 -0.11038208]\n",
      " [-0.06484985  0.09588623  0.11465454 -0.11038208]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Layerı kullanmak için input gönderelim \n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c63016-99eb-4973-b4b0-41021f2fbc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.06484985 -0.02227783  0.06439209 -0.01803589]\n",
      " [-0.06484985 -0.02227783  0.06439209 -0.01803589]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Yukarıdaki sınıf add_weight() metoduyla bu şekilde de tanımlanabilir. \n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8b62e-b2a4-4560-9d8b-3138b885f57a",
   "metadata": {},
   "source": [
    "## Layerlarda eğitilemez ağırlıklar olabilir\n",
    "Eğitilebilir ağırlıkların yanında eğitilemez ağırlıklarda vardır. Bu ağırlıklar backprop sırasında hesaba katılmazlar.  \n",
    "Nasıl eklediğimizi inceleyelim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cebbce9-a34e-4577-a4dd-a43a7fe211d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self,input_dim):\n",
    "        super(ComputeSum,self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),trainable=False)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        # tf.reduce_sum(x,axis=0) --> satırları toplar [[1,1],[1,1]] = [2,2] \n",
    "        self.total.assign_add(tf.reduce_sum(inputs,axis=0)) # totale value ekler. total 0 olduğu için değişim olmaz\n",
    "        return self.total\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9701f8-2316-4319-9077-a8e4481f5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ağırlıklar :  1\n",
      "Non-trainable Ağırlıklar:  1\n",
      "trainable Ağırlıklar: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Ağırlıklar : \",len(my_sum.weights))\n",
    "print(\"Non-trainable Ağırlıklar: \",(len(my_sum.non_trainable_weights)))\n",
    "print(\"trainable Ağırlıklar:\", my_sum.trainable_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd62435-18bf-4589-a1e9-5acb926d5c26",
   "metadata": {},
   "source": [
    "## Input shape öğrenilene kadar weigthleri oluşturmayı ertelemek\n",
    "Yukarıdaki örneklerde weightler init metodu içinde oluşturduk. Bunun yerine weightleri oluşturmayı build metodu içerisined yapmak isteriz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54efa9c3-4301-4a14-97c7-7ea22f486bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self,units=32):\n",
    "        super(Linear,self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self,input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer=\"random_normal\",trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),initializer=\"random_normal\",trainable=True)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3307f90a-ec6e-40ae-ac03-10313cc0dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __call__ metodu layer oluşturup input gönderdiğimizde çağrılır. \n",
    "linear_layer = Linear(32)\n",
    "# layer çağrıldı ve input gönderildi. weightler otomatik olarak oluşturulu. \n",
    "y = linear_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac66353-b29c-458a-bf56-013cf656fd96",
   "metadata": {},
   "source": [
    "## Layerlar recursive olarak kullanılabilir\n",
    "Eğer bir layerı diğer bir layera gönderirsek dışta bulunan layer içerdeki layerın weightlerini takip edebilir.  \n",
    "Sublayerları init metodu içeriside yapmak önerilir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da0afba1-ba3e-4e89-9462-c52855f97634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f0a756-8810-4ad8-81d2-5d58d0ee0e1a",
   "metadata": {},
   "source": [
    "## The add_loss() method \n",
    "Bir layer için call metodu yazdığımızda, daha sonra kullanmak isteyeceğimiz loss tensorlarını oluşturabiliriz. Bunu self.add_loss(value) ile sağlayabiliriz.   \n",
    "Loss değerlerini layer.losses ile getirebiliriz. Bu değerler her __call__() metodu çağrıldığında resetlenir. Bu sebeple layers.losses her zaman son forward pass da ki loss değerlerinir tutar. Loss değerleri her layerda alınır ve total loss'a eklenir.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d3bc591-4795-4e24-94c3-f946c5e8ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self,rate=1e-2):\n",
    "        super(ActivityRegularizationLayer,self).__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        self.add_loss(self.rate*tf.reduce_sum(inputs))\n",
    "        return inputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "346ba024-18d0-4848-8abe-258ef775f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dd15481-a744-44a4-8f56-1f419db97c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0 # herhangi bir loss oluşturulmadı\n",
    "\n",
    "_ = layer(tf.zeros(1,1))\n",
    "assert len(layer.losses) == 1 # bir loss değeri oluştu __call__ metodu çağrıldı \n",
    "\n",
    "# layer.losses değeri her __call__ metodu çağrıldığında resetlenir. \n",
    "_ = layer(tf.zeros(1,1)) \n",
    "assert len(layer.losses) == 1 # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287bc11-eb34-4573-bf1b-9b506fc47d8b",
   "metadata": {},
   "source": [
    "## add_metric method()\n",
    "Layerlar add_loss yanında add_metric metoduna da sahiptir. Model başarısının takip edilmesi için gereklidir. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2db9a29c-71f1-435c-8970-3f2c80023deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "    def call(self,targets,logits,sample_weights=None):\n",
    "        # Eğitim sırasındaki loss değeri hesaplanır ve add.loss ile eklenir.\n",
    "        loss = self.loss_fn(targets,logits,sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # Accuracy hesaplanır ve eklenir \n",
    "        acc = self.accuracy_fn(targets,logits,sample_weights)\n",
    "        self.add_metric(acc,name=\"accuracy\")\n",
    "        \n",
    "        # .predict sonucunu döndürür\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24dad5b1-092c-4098-9302-ced429ddcca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.metric :  [<keras.metrics.BinaryAccuracy object at 0x000001FA3BE113C8>]\n",
      "accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "# metricleri takip etmek için layer.metrics kullanılır \n",
    "\n",
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2,2))\n",
    "logits = tf.ones((2,2))\n",
    "y = layer(targets,logits)\n",
    "\n",
    "print(\"layer.metric : \",layer.metrics)\n",
    "print(\"accuracy : \",float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fb7338f-98bc-4ebc-9a6b-760eaab7fc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 259ms/step - loss: 0.9585 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa4a50b940>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add_loss gibi metriclerde fit metodu ile takip edilebilir \n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs,targets],outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "data = {\n",
    "    \"inputs\":np.random.random((3,3)),\n",
    "    \"targets\":np.random.random((3,10)),\n",
    "}\n",
    "\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048784bf-0ab4-4b64-8940-14d367e9a219",
   "metadata": {},
   "source": [
    "## Layerlarda serialization yapmak için \n",
    "get_config() metodunu implemente edersek layerları seralize edebiliriz. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59491ff8-bfa9-4885-962c-3ff1ee59ee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self,units=32):\n",
    "        super(Linear,self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.w = self.add_weights(shape=(input_shape[-1],self.units),\n",
    "                                 initializer=\"random_normal\",trainable=True)\n",
    "        self.b = self.add_weights(shape=(self.units,),initializer=\"random_normal\",trainable=True)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"units\":self.units}\n",
    "    \n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad646958-893b-41c1-a495-17b7acd61e33",
   "metadata": {},
   "source": [
    "Base layerdaki init metodu name dtype gibi bazı parametreler alır. Bu bilgileri config içerisine göndermek daha uygun olacaktır. Eğer serialization işleminde daha flex olmamız gerekiyorsa from_config metodunu da override edebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74ad4c7b-8700-4b94-bdfa-a1d5c7dac6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_9', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fea14-1018-4a0f-a14e-945f188bd1f2",
   "metadata": {},
   "source": [
    "## Training durumunda call metodu \n",
    "BatchNormalization, Dropout gibi bazı layerler eğitim ve inference aşamalarında farklı davranışlar göstermelidir. Bunu sağlayabilmek için boolean bir training parametresi eklemek daha iyi olacaktır. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87fb7a98-a24f-42ce-b0d7-d95fa2bf9468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # eğitim sırasında dropout uygulanır ama inference aşamasında dropout uygulanmamalı \n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547fdf00-7e75-4f03-b5c3-afda18c849a4",
   "metadata": {},
   "source": [
    "# Model Sınıfı \n",
    "Layer sınıfında iç hesaplamaları yaparız. Model sınıfını kullanarakta modelin yapısını oluştururuz. Örneğin ResNet50 modelinde bulunan ResNet bloklaro Layer sınıfından extend edilmiştir. Genel bir Model sınıfı kullanarakta ResNet50 modelini oluştururuz.  \n",
    "Model sınıfı Layer sınıfının özelliklerini barındırmakla birlikte ayrıca  ;\n",
    "* model.fit , model.evaluate ve model.predict metotları bulunur\n",
    "* layerları görmek için model.layers kullanılır \n",
    "* Modeli kaydetmek ve searalize etmek için model.save ve save_weights metotları bulunur. \n",
    "\n",
    "Layer sınıfı literatürde neye layer diyorsak bunları karşılar. Örneğin ; convolution layer, recurrent layer yada bloklar.  \n",
    "Model sınıfı ise literatürde neye model diyorsak buna karşılık gelir. Örneğin ; deep learning model, network   \n",
    "\n",
    "Model mi layer sınıfını mı kullanmam gerek gibi bir soru ile karşı karşıya kalırsanız. fit metodunu çağırmaya, save metoduna ihityacım varmı sorularını sorarak karar verebilirsiniz.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19ef46f2-a257-4268-b9c0-73e539936f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# resnet = ResNet()\n",
    "# dataset = ...\n",
    "# resnet.fit(dataset, epochs=10)\n",
    "# resnet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196c188-2be4-4c02-840f-a1dbef6d8bbf",
   "metadata": {},
   "source": [
    "## Tüm öğrendiklerimi bir araya getirelim \n",
    "* Layer sınıfı içerisinde weights, bias bilgileri bulunur __call__ metodu içerisinde hesaplamalar yapılır \n",
    "* Layerlar recursive olarak kullanılabilir. \n",
    "* Layerlar ile loss ve accuracy bilgisi kontrol edilebilir \n",
    "* Tüm layerları kaplayan yapı Modeldir. \n",
    "\n",
    "Variational AutoEncoder ile tüm bu öğrendiklerimizi bir araya getirelim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32128b76-aef0-47f7-9424-a3be8e069f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "709aeb12-4c45-4dd9-8292-8a1b391a1809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 0.3431\n",
      "step 100: mean loss = 0.1250\n",
      "step 200: mean loss = 0.0989\n",
      "step 300: mean loss = 0.0890\n",
      "step 400: mean loss = 0.0841\n",
      "step 500: mean loss = 0.0808\n",
      "step 600: mean loss = 0.0787\n",
      "step 700: mean loss = 0.0771\n",
      "step 800: mean loss = 0.0759\n",
      "step 900: mean loss = 0.0749\n",
      "Start of epoch 1\n",
      "step 0: mean loss = 0.0746\n",
      "step 100: mean loss = 0.0740\n",
      "step 200: mean loss = 0.0735\n",
      "step 300: mean loss = 0.0730\n",
      "step 400: mean loss = 0.0727\n",
      "step 500: mean loss = 0.0723\n",
      "step 600: mean loss = 0.0720\n",
      "step 700: mean loss = 0.0717\n",
      "step 800: mean loss = 0.0715\n",
      "step 900: mean loss = 0.0712\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd097907-b385-409a-b83f-766f3e2ded8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0747\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa4a718a90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575827ef-22a4-4d53-9de7-a7b1a394003a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tod",
   "language": "python",
   "name": "tod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
